---
apiVersion: v1
kind: ConfigMap
metadata:
  name: uksrc-ral-cluster-config
  namespace: capi-self
data:
  values.yaml: |
    # Must match the name of the (sealed) secret in credentials.yaml
    cloudCredentialsSecretName: uksrc-ral-cluster-credentials

    kubernetesVersion: 1.30.6
    machineImageId: 16a8b936-4fa7-41c7-a512-dfc6d8d2ea34

    clusterNetworking:
      externalNetworkId: 5283f642-8bd8-48b6-8608-fa3006ff4539
      internalNetwork:
        networkFilter:
          id: e04ec697-e979-487c-929d-3c92893f15df
          name: UKSRC-Network

    controlPlane:
      machineFlavor: l3.nano
      machineCount: 3

    nodeGroups:
      - name: workers
        machineFlavor: l3.nano
        autoscale: true
        machineCountMin: 8
        machineCountMax: 10

      - name: workers-small
        machineFlavor: l3.small
        autoscale: true
        machineCountMin: 8
        machineCountMax: 8

      - name: workers-micro
        machineFlavor: l3.micro
        autoscale: true
        machineCountMin: 8
        machineCountMax: 12

      # - name: workers-l6.c32
      #   machineFlavor: l6.c32
      #   autoscale: true
      #   machineCountMin: 10
      #   machineCountMax: 20       

      # - name: workers-l6.c64
      #   machineFlavor: l6.c64
      #   autoscale: true
      #   machineCountMin: 2
      #   machineCountMax: 20 

      # - name: workers-l6.c124
      #   machineFlavor: l6.c124
      #   autoscale: true
      #   machineCountMin: 2
      #   machineCountMax: 20

      # - name: workers-l6.c252
      #   machineFlavor: l6.c252
      #   autoscale: true
      #   machineCountMin: 2
      #   machineCountMax: 20

    # Configuration for the cluster autoscaler
    autoscaler:
      # The image to use for the autoscaler component
      image:
        repository: registry.k8s.io/autoscaling/cluster-autoscaler
        pullPolicy: IfNotPresent
        tag: v1.30.0
      imagePullSecrets: []
      # Any extra args for the autoscaler
      extraArgs:
        # Make sure logs go to stderr
        logtostderr: true
        stderrthreshold: info
        # Output at a decent log level
        v: 4
        # Cordon nodes before terminating them so new pods are not scheduled there
        cordon-node-before-terminating: "true"
        # When scaling up, choose the node group that will result in the least idle CPU after
        expander: least-waste,random
        # Allow pods in kube-system to prevent a node from being deleted
        skip-nodes-with-system-pods: "true"
        # Allow pods with emptyDirs to be evicted
        skip-nodes-with-local-storage: "false"
        # Allow pods with custom controllers to be evicted
        skip-nodes-with-custom-controller-pods: "false"

    apiServer:
      # Indicates whether to deploy a load balancer for the API server
      enableLoadBalancer: true
      # Indicates what loadbalancer provider to use. Default is amphora
      loadBalancerProvider:
      # Restrict loadbalancer access to select IPs
      # allowedCidrs
      #  - 192.168.0.0/16  # needed for cluster to init
      #  - 10.10.0.0/16  # IPv4 Internal Network
      #  - 123.123.123.123 # some other IPs
      # Indicates whether to associate a floating IP with the API server
      associateFloatingIP: true
      # The specific floating IP to associate with the API server
      # If not given, a new IP will be allocated if required
      floatingIP: 130.246.215.245
      # The specific fixed IP to associate with the API server
      # If enableLoadBalancer is true, this will become the VIP of the load balancer
      # If enableLoadBalancer and associateFloatingIP are both false, this should be
      # the IP of a pre-allocated port to be used as the VIP
      fixedIP:
      # The port to use for the API server
      port: 6443

    addons:
      # Use the cilium CNI
      cni:
        type: cilium

      # Enable the monitoring stack
      monitoring:
        enabled: true

      # Disable NFD and the NVIDIA/Mellanox operators
      nodeFeatureDiscovery:
        enabled: false
      nvidiaGPUOperator:
        enabled: false
      mellanoxNetworkOperator:
        enabled: false

      # set availabilty zone as upstream uses nova by default
      openstack:
        csiCinder:
          defaultStorageClass:
            availabilityZone: ceph
        csiManila:
          enabled: true

      ingress:
        enabled: true
        nginx:
          release:
            values:
              controller:
                service:
                  loadBalancerIP: "130.246.81.193"

      csi:
        cephfs:
          enabled: true



    kubeNetwork:
      # By default, use the private network range 10.0.0.0/12 for the cluster network
      # We split it into two equally-sized blocks for pods and services
      # This gives ~500,000 addresses in each block and distinguishes it from
      # internal net on 172.16.x.y
      pods:
        cidrBlocks:
          - 10.0.0.0/13
      services:
        cidrBlocks:
          - 10.8.0.0/13
      serviceDomain: cluster.local

    # Settings for registry mirrors
    registryMirrors: { docker.io: ["https://dockerhub.stfc.ac.uk"] }



